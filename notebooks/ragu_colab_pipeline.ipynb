{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAGU Baseline Replication on Google Colab\n",
        "\n",
        "This notebook orchestrates the end-to-end pipeline described in *Uncertainty Quantification in Retrieval-Augmented Question Answering (Perez et al., 2024)*. It mirrors the steps used in the paper so you can reproduce the retrieval-augmented QA baseline, generate stochastic samples, and evaluate uncertainty metrics (ECE, AUROC, semantic entropy, etc.) inside a Colab runtime.\n",
        "\n",
        "> **Tip:** The full pipeline is GPU intensive. Use a Colab runtime with an A100 or higher-memory GPU (Colab Pro/Pro+) and plenty of disk space (>150\u202fGB) if you plan to download the full DPR corpus and run large language models such as Qwen2-72B.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Runtime diagnostics\n",
        "Check the attached GPU before proceeding. You need a recent NVIDIA GPU (A100/H100 class) to load the large AWQ quantized models used in the paper. For debugging or sanity checks you can temporarily switch to a lighter model (e.g., `Qwen/Qwen2-7B-Instruct`).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Python dependencies\n",
        "This installs the exact toolchain used across the repository:\n",
        "\n",
        "* `vllm` for fast autoregressive decoding.\n",
        "* `contriever` for dense retrieval.\n",
        "* `lm-polygraph`, `xgboost`, `wandb`, and metric toolkits required by the uncertainty scripts.\n",
        "* `faiss-gpu` (preferred) or `faiss-cpu` for passage search; switch to CPU if your runtime lacks CUDA headers.\n",
        "\n",
        "> **Note:** Colab runtimes start with an older version of `pip`. Upgrading it first avoids resolver errors when installing `vllm` wheels.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "python -m pip install --upgrade pip\n",
        "python -m pip install --no-cache-dir   accelerate==0.32.1   bitsandbytes==0.43.1   datasets==2.20.0   faiss-gpu==1.7.4   git+https://github.com/facebookresearch/contriever.git   git+https://github.com/EleutherAI/lm-polygraph.git   huggingface_hub==0.23.4   jsonlines==4.0.0   numpy==1.26.4   pandas==2.2.2   pyarrow==16.1.0   scikit-learn==1.5.1   sentencepiece==0.2.0   torch==2.3.1   torchvision==0.18.1   torchaudio==2.3.1   transformers==4.42.3   vllm==0.4.2   wandb==0.17.4   xgboost==2.1.1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure runtime-wide environment variables\n",
        "W&B logging is disabled by default to keep the pipeline self-contained, and tokenizer parallelism warnings are silenced. Adjust these flags if you prefer online tracking.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ.setdefault(\"WANDB_MODE\", \"offline\")\n",
        "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
        "print(\"WANDB_MODE=\", os.environ[\"WANDB_MODE\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Clone the RAGU and Contriever repositories\n",
        "The paper couples this repository with Facebook AI's Contriever retriever. Both repos are cloned into `/content` (the default working directory in Colab). Re-run these cells only if you want a fresh checkout.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "WORKSPACE = Path(\"/content\")\n",
        "RAGU_REPO = \"https://github.com/lauhaide/ragu.git\"\n",
        "CONTRIEVER_REPO = \"https://github.com/facebookresearch/contriever.git\"\n",
        "\n",
        "RAGU_DIR = WORKSPACE / \"ragu\"\n",
        "CONTRIEVER_DIR = WORKSPACE / \"contriever\"\n",
        "\n",
        "if not RAGU_DIR.exists():\n",
        "    subprocess.run([\"git\", \"clone\", \"--depth\", \"1\", RAGU_REPO, str(RAGU_DIR)], check=True)\n",
        "else:\n",
        "    print(\"RAGU repo already present at\", RAGU_DIR)\n",
        "\n",
        "if not CONTRIEVER_DIR.exists():\n",
        "    subprocess.run([\"git\", \"clone\", \"--depth\", \"1\", CONTRIEVER_REPO, str(CONTRIEVER_DIR)], check=True)\n",
        "else:\n",
        "    print(\"Contriever repo already present at\", CONTRIEVER_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create directories for inputs (retrieval-ready datasets) and outputs (LLM generations, metrics). These paths mirror the `${HOMEDATA}` and `${HOMEOUT}` variables used in the original shell scripts.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = Path(\"/content/ragu_data\")\n",
        "OUTPUT_DIR = Path(\"/content/ragu_outputs\")\n",
        "CACHE_DIR = Path(\"/content/ragu_cache\")\n",
        "for path in (DATA_DIR, OUTPUT_DIR, CACHE_DIR):\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"DATA_DIR:\", DATA_DIR)\n",
        "print(\"OUTPUT_DIR:\", OUTPUT_DIR)\n",
        "print(\"CACHE_DIR:\", CACHE_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Switch the working directory to the cloned RAGU repo so that relative imports (e.g., `../utils`) resolve exactly as expected by the authors' scripts.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(RAGU_DIR)\n",
        "print(\"Current working directory:\", os.getcwd())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Authenticate with Hugging Face (optional but recommended)\n",
        "Large checkpoint downloads (Qwen2-72B, Llama 3.1, etc.) require an access token tied to your Hugging Face account. Uncomment and run the login cell, or set the `HF_TOKEN` environment variable beforehand.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# from huggingface_hub import login\n",
        "# login(token=\"hf_...\", add_to_git_credential=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Prepare QA data in the expected JSONL format\n",
        "The helper below pulls a slice of Natural Questions (`nq_open`) via \ud83e\udd17 Datasets, converts it to the RAGU schema (question, answers, q_id), and saves it under `${DATA_DIR}`. Replace the dataset loader if you already have DPR-formatted files.\n",
        "\n",
        "* `DATASET_NAME`: identifier used downstream.\n",
        "* `SPLIT`: typically `train`, `dev`, or `test`.\n",
        "* `SAMPLE_SIZE`: shrink this during dry runs to conserve GPU time.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import jsonlines\n",
        "from pathlib import Path\n",
        "\n",
        "DATASET_NAME = \"nq\"\n",
        "SPLIT = \"dev\"\n",
        "SAMPLE_SIZE = 200  # set to None to keep the full split\n",
        "\n",
        "raw_split = load_dataset(\"nq_open\", split=\"validation\")\n",
        "if SAMPLE_SIZE is not None:\n",
        "    raw_split = raw_split.select(range(SAMPLE_SIZE))\n",
        "\n",
        "output_path = DATA_DIR / f\"{DATASET_NAME}-{SPLIT}.jsonl\"\n",
        "with jsonlines.open(output_path, mode=\"w\") as writer:\n",
        "    for idx, row in enumerate(raw_split):\n",
        "        answers = row[\"answer\"] if isinstance(row[\"answer\"], list) else [row[\"answer\"]]\n",
        "        writer.write({\n",
        "            \"question\": row[\"question\"],\n",
        "            \"answers\": answers,\n",
        "            \"q_id\": f\"{DATASET_NAME}-{SPLIT}-{idx}\"\n",
        "        })\n",
        "\n",
        "print(f\"Saved {len(raw_split)} examples to {output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Stage dense retrieval assets (DPR Wikipedia)\n",
        "Contriever expects the DPR-formatted Wikipedia passages plus the pre-computed embeddings released by the original authors. These files collectively exceed 80\u202fGB, so mount Google Drive or attach a persistent disk before enabling the download flag.\n",
        "\n",
        "Set the boolean switches below to `True` the first time you run the notebook. Subsequent runs will skip the downloads if the files already exist.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import shlex\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "psgs_tsv = DATA_DIR / \"psgs_w100.tsv\"\n",
        "embeddings_dir = DATA_DIR / \"wikipedia_embeddings\"\n",
        "\n",
        "DOWNLOAD_PASSAGES = False  # <-- change to True for the full 21M-passage corpus (~2.7 GB compressed)\n",
        "DOWNLOAD_EMBEDDINGS = False  # <-- change to True for the pre-built FAISS index (~80 GB)\n",
        "\n",
        "if DOWNLOAD_PASSAGES and not psgs_tsv.exists():\n",
        "    url = \"https://dl.fbaipublicfiles.com/dpr/wikipedia_split/psgs_w100.tsv.gz\"\n",
        "    subprocess.run(shlex.split(f\"wget {url} -O {psgs_tsv}.gz\"), check=True)\n",
        "    subprocess.run([\"gunzip\", f\"{psgs_tsv}.gz\"], check=True)\n",
        "else:\n",
        "    print(\"Skipping passage download (toggle DOWNLOAD_PASSAGES to True).\")\n",
        "\n",
        "if DOWNLOAD_EMBEDDINGS and not embeddings_dir.exists():\n",
        "    embeddings_dir.mkdir(parents=True, exist_ok=True)\n",
        "    url = \"https://dl.fbaipublicfiles.com/contriever/embeddings/contriever-msmarco/wikipedia_embeddings.tar\"\n",
        "    subprocess.run(shlex.split(f\"wget {url} -O {embeddings_dir}.tar\"), check=True)\n",
        "    subprocess.run([\"tar\", \"-xf\", f\"{embeddings_dir}.tar\", \"-C\", str(DATA_DIR)], check=True)\n",
        "    Path(f\"{embeddings_dir}.tar\").unlink()\n",
        "else:\n",
        "    print(\"Skipping embedding download (toggle DOWNLOAD_EMBEDDINGS to True).\")\n",
        "\n",
        "print(\"Passage file exists:\", psgs_tsv.exists())\n",
        "print(\"Embeddings directory exists:\", embeddings_dir.exists())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you do **not** have the official index, you can still smoke-test the pipeline by building a miniature FAISS index over a few thousand passages. Uncomment the block below to create a toy corpus directly inside Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "# import numpy as np\n",
        "# import faiss\n",
        "# import torch\n",
        "# from transformers import AutoTokenizer, AutoModel\n",
        "#\n",
        "# MINI_CORPUS_SIZE = 5000\n",
        "# CORPUS_PATH = DATA_DIR / \"mini_psgs.tsv\"\n",
        "#\n",
        "# if not CORPUS_PATH.exists():\n",
        "#     wiki = load_dataset(\"wiki_dpr\", \"psgs_w100\", split=f\"train[:{MINI_CORPUS_SIZE}]\")\n",
        "#     with CORPUS_PATH.open(\"w\") as fout:\n",
        "#         for row in wiki:\n",
        "#             fout.write(f\"{row['id']}\t{row['title']}\t{row['text']}\n",
        "\")\n",
        "#     print(f\"Mini corpus saved to {CORPUS_PATH}\")\n",
        "# else:\n",
        "#     print(\"Mini corpus already exists at\", CORPUS_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Run Contriever retrieval\n",
        "Point the retriever at your JSONL file. When using the full DPR assets, set `PASSAGES_TSV` to `psgs_w100.tsv` and `EMB_PATTERN` to `wikipedia_embeddings/*`. For the miniature corpus, swap in `mini_psgs.tsv` and drop the `--passages_embeddings` argument to trigger on-the-fly encoding.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "PASSAGES_TSV = psgs_tsv if psgs_tsv.exists() else DATA_DIR / \"mini_psgs.tsv\"\n",
        "EMB_PATTERN = str(embeddings_dir / \"*\") if embeddings_dir.exists() else \"\"\n",
        "OUTPUT_PREFIX = DATA_DIR / f\"{DATASET_NAME}-{SPLIT}-ctx20\"\n",
        "OUTPUT_PREFIX.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "cmd = [\n",
        "    \"python\",\n",
        "    str((Path(\"/content\") / \"contriever\" / \"passage_retrieval.py\")),\n",
        "    \"--model_name_or_path\", \"facebook/contriever-msmarco\",\n",
        "    \"--passages\", str(PASSAGES_TSV),\n",
        "    \"--data\", str(output_path),\n",
        "    \"--output_dir\", str(OUTPUT_PREFIX),\n",
        "    \"--n_docs\", \"20\",\n",
        "]\n",
        "\n",
        "if EMB_PATTERN:\n",
        "    cmd.extend([\"--passages_embeddings\", EMB_PATTERN])\n",
        "\n",
        "print(\"Running:\", \" \".join(cmd))\n",
        "subprocess.run(cmd, check=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The retriever writes a JSONL file that augments each example with a `ctxs` list. Confirm the schema before moving on.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import jsonlines\n",
        "from itertools import islice\n",
        "\n",
        "retrieved_file = OUTPUT_PREFIX / f\"{DATASET_NAME}-{SPLIT}.jsonl\"\n",
        "print(\"Retrieved file:\", retrieved_file)\n",
        "with jsonlines.open(retrieved_file) as reader:\n",
        "    for record in islice(reader, 2):\n",
        "        print({k: record[k] for k in ['question', 'answers', 'ctxs']})\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Generate most-likely RAG answers with vLLM\n",
        "Use the same CLI that the paper employs (`retrieval_qa/run_baseline_lm.py`). Edit `MODEL_NAME` to whichever checkpoint fits your GPU budget.\n",
        "\n",
        "* For faithful replication: `Qwen/Qwen2-72B-Instruct-AWQ` with AWQ quantization.\n",
        "* For smoke tests: `Qwen/Qwen2-7B-Instruct` or `meta-llama/Meta-Llama-3-8B-Instruct`.\n",
        "\n",
        "The script appends model predictions, token log-probs, PMI statistics, and other diagnostic fields directly into the retrieved JSONL file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "MODEL_NAME = \"Qwen/Qwen2-7B-Instruct\"\n",
        "RESULT_FILE = OUTPUT_DIR / f\"{MODEL_NAME.split('/')[-1]}-{DATASET_NAME}-{SPLIT}-RAGQA.jsonl\"\n",
        "\n",
        "cmd = [\n",
        "    \"python\", \"retrieval_qa/run_baseline_lm.py\",\n",
        "    \"--model_name\", MODEL_NAME,\n",
        "    \"--split\", SPLIT,\n",
        "    \"--input_file\", str(retrieved_file),\n",
        "    \"--result_fp\", str(RESULT_FILE),\n",
        "    \"--prompt_name\", \"chat_directRagQA_REAR3\",\n",
        "    \"--chat_template\",\n",
        "    \"--top_n\", \"5\",\n",
        "    \"--temperature\", \"0.0\",\n",
        "    \"--top_p\", \"1\",\n",
        "    \"--max_new_tokens\", \"50\",\n",
        "    \"--do_stop\",\n",
        "    \"--logprobs\", \"1\",\n",
        "    \"--compute_pmi\",\n",
        "]\n",
        "\n",
        "print(\"Running:\", \" \".join(cmd))\n",
        "subprocess.run(cmd, check=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspect a sample prediction to verify that generations, PMI terms, and Fisher-Rao statistics were appended correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "with jsonlines.open(RESULT_FILE) as reader:\n",
        "    sample = next(reader)\n",
        "\n",
        "print(\"Fields:\", list(sample.keys()))\n",
        "print(\"Predicted answer:\", sample.get(\"predicted_answer\"))\n",
        "print(\"Acc_LM (token-level exact match):\", sample.get(\"acc\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluate answers with the LLM-based accuracy judge\n",
        "The paper uses `Qwen/Qwen2-72B-Instruct-AWQ` as the evaluator (`run_compute_accLM.py`). For lighter runs, you can substitute a smaller instruction-tuned model. Set `--eval_distil` when evaluating per-passage generations; omit it for single-answer files.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "EVAL_MODEL = \"Qwen/Qwen2-7B-Instruct\"\n",
        "ACC_RESULT_FILE = OUTPUT_DIR / f\"{RESULT_FILE.stem}-acc.jsonl\"\n",
        "\n",
        "cmd = [\n",
        "    \"python\", \"retrieval_qa/run_compute_accLM.py\",\n",
        "    \"--model_name\", EVAL_MODEL,\n",
        "    \"--input_file\", str(RESULT_FILE),\n",
        "    \"--result_fp\", str(ACC_RESULT_FILE),\n",
        "    \"--acc\",\n",
        "    \"--top_n\", \"5\",\n",
        "]\n",
        "\n",
        "print(\"Running:\", \" \".join(cmd))\n",
        "subprocess.run(cmd, check=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Sample stochastic generations for semantic uncertainty\n",
        "`semantic_uncertainty/generate.py` replicates Semantic Entropy, PMI, Fisher-Rao, and other estimators by drawing multiple answers per question. Keep `NUM_GENERATIONS` modest in Colab to cap inference cost.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "SAMPLES_FILE = OUTPUT_DIR / f\"{MODEL_NAME.split('/')[-1]}-{DATASET_NAME}-{SPLIT}-samples.jsonl\"\n",
        "\n",
        "cmd = [\n",
        "    \"python\", \"semantic_uncertainty/generate.py\",\n",
        "    \"--model\", MODEL_NAME,\n",
        "    \"--input_file\", str(retrieved_file),\n",
        "    \"--result_fp\", str(SAMPLES_FILE),\n",
        "    \"--prompt_name\", \"chat_directRagQA_REAR3\",\n",
        "    \"--chat_template\",\n",
        "    \"--top_n\", \"5\",\n",
        "    \"--split\", SPLIT,\n",
        "    \"--max_new_tokens\", \"50\",\n",
        "    \"--do_stop\",\n",
        "    \"--num_generations\", \"5\",\n",
        "]\n",
        "\n",
        "print(\"Running:\", \" \".join(cmd))\n",
        "subprocess.run(cmd, check=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Compute uncertainty metrics and calibration curves\n",
        "`semantic_uncertainty/generate_answers.py` aggregates the outputs above into AUROC, AURAC, ECE, and Semantic Entropy scores. Because it also supports Passage Utility predictors, toggle `COMPUTE_UTILITY` off unless you have already trained those models.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "UNCERTAINTY_DIR = OUTPUT_DIR / \"uncertainty_runs\"\n",
        "UNCERTAINTY_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "cmd = [\n",
        "    \"python\", \"semantic_uncertainty/generate_answers.py\",\n",
        "    \"--dataset\", DATASET_NAME,\n",
        "    \"--precomputed_gen\",\n",
        "    \"--no-get_training_set_generations\",\n",
        "    \"--no-get_training_set_generations_most_likely_only\",\n",
        "    \"--eval_mode\", SPLIT,\n",
        "    \"--most_likely_file\", str(ACC_RESULT_FILE),\n",
        "    \"--samples_file\", str(SAMPLES_FILE),\n",
        "    \"--original_file\", str(output_path),\n",
        "    \"--top_n\", \"5\",\n",
        "    \"--acc_LM\",\n",
        "    \"--metric\", \"llm\",\n",
        "    \"--entity\", \"offline\",\n",
        "    \"--experiment_lot\", \"colab\",\n",
        "    \"--num_samples\", \"100\",\n",
        "    \"--compute_uncertainties\",\n",
        "    \"--no-compute_utility\",\n",
        "]\n",
        "\n",
        "print(\"Running:\", \" \".join(cmd))\n",
        "subprocess.run(cmd, check=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Inspect the resulting calibration metrics\n",
        "The uncertainty script writes pickle files (`uncertainty_measures.pkl`, `experiment_details.pkl`) and logs summary scores. Load them directly to verify AUROC/ECE values against the numbers reported in the paper.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "measures_path = UNCERTAINTY_DIR / \"uncertainty_measures.pkl\"\n",
        "if measures_path.exists():\n",
        "    with measures_path.open(\"rb\") as fin:\n",
        "        measures = pickle.load(fin)\n",
        "    print(\"Available uncertainty measures:\", list(measures.get(\"uncertainty_measures\", {}).keys()))\n",
        "else:\n",
        "    print(\"Could not find\", measures_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Next steps\n",
        "* Increase `SAMPLE_SIZE` and `--num_generations` once the pipeline works end-to-end.\n",
        "* Swap in the exact checkpoints from the paper (`Qwen/Qwen2-72B-Instruct-AWQ`, `gemma-2-9b-it`, etc.) and rerun the evaluation cells.\n",
        "* Re-enable `--compute_utility` with Passage Utility annotations when you are ready to benchmark BR-RAG against the published baselines.\n",
        "\n",
        "Happy experimenting!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}