{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGU Colab Pipeline (A100 GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through the exact sequence of scripts used in the RAGU repository to recreate the paper's retrieval-augmented question answering pipeline on Google Colab.\n",
    "\n",
    "> **Important:** In Colab select **Runtime \u2192 Change runtime type \u2192 GPU \u2192 A100** before running the notebook. A different GPU may not have enough VRAM for the vLLM generation steps."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Docker Engine in Colab\n",
    "Installing Docker lets us run the pre-built `lauhaide/algomo:v1` image so we don't have to manage Python packages manually. The commands below install Docker, start the daemon, and confirm that it is available in the Colab runtime."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "apt-get update\n",
    "apt-get install -y docker.io\n",
    "service docker start\n",
    "docker --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** If Docker fails to start (for example with `Cannot connect to the Docker daemon`), restart the Colab runtime and rerun the setup cells above before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --gpus all --rm nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone repositories and create working directories"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "cd /content\n",
    "if [ ! -d ragu ]; then\n",
    "  git clone https://github.com/lauraperez/ragu.git\n",
    "fi\n",
    "if [ ! -d contriever ]; then\n",
    "  git clone https://github.com/facebookresearch/contriever.git\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "BASE_DIR = Path('/content')\n",
    "RAGU_DIR = BASE_DIR / 'ragu'\n",
    "CONTRIEVER_DIR = BASE_DIR / 'contriever'\n",
    "DATA_DIR = RAGU_DIR / 'colab_data'\n",
    "OUTPUT_DIR = RAGU_DIR / 'colab_outputs'\n",
    "CACHE_DIR = BASE_DIR / 'ragu_cache'\n",
    "\n",
    "for path in (DATA_DIR, OUTPUT_DIR, CACHE_DIR):\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "os.chdir(RAGU_DIR)\n",
    "print('Working directory set to:', RAGU_DIR)\n",
    "print('Cache directory:', CACHE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Provide an optional Hugging Face token\n",
    "If a model requires authentication, store your token in the `HF_TOKEN` environment variable so the Docker container can access it. Public models can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if not os.environ.get('HF_TOKEN'):\n",
    "    token = getpass('Enter Hugging Face token (leave blank to skip): ').strip()\n",
    "    if token:\n",
    "        os.environ['HF_TOKEN'] = token\n",
    "        print('Stored HF token in the environment for this session.')\n",
    "    else:\n",
    "        print('Proceeding without a Hugging Face token.')\n",
    "else:\n",
    "    print('Using Hugging Face token from environment.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build the retrieval dataset input\n",
    "We fetch the `nq_open` validation split and convert it into the DPR-style format expected by `create_retrieval_data.py`. The entire process happens inside the Docker image so we reuse its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "# Generate raw DPR-style JSON inside the container\n",
    "docker run --gpus all --rm \\\n",
    "  -e HF_TOKEN=\"${HF_TOKEN}\" \\\n",
    "  -v /content/ragu:/workspace/ragu \\\n",
    "  -w /workspace/ragu \\\n",
    "  lauhaide/algomo:v1 \\\n",
    "  /bin/bash -lc \"python - <<'PY'\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "base = Path('colab_data')\n",
    "base.mkdir(parents=True, exist_ok=True)\n",
    "raw_path = base / 'nq_open_validation_raw.json'\n",
    "ds = load_dataset('nq_open', split='validation')\n",
    "records = []\n",
    "for idx, example in enumerate(ds):\n",
    "    records.append({\n",
    "        'question': example['question'],\n",
    "        'short_answers': example['answer'],\n",
    "        'example_id': f'nq-open-{idx}'\n",
    "    })\n",
    "raw_path.write_text(json.dumps({'data': records}))\n",
    "print('Saved raw DPR JSON to', raw_path)\n",
    "PY\"\n",
    "\n",
    "docker run --gpus all --rm \\\n",
    "  -v /content/ragu:/workspace/ragu \\\n",
    "  -w /workspace/ragu/data_creation \\\n",
    "  lauhaide/algomo:v1 \\\n",
    "  python create_retrieval_data.py \\\n",
    "    --dataset NQ \\\n",
    "    --input_file /workspace/ragu/colab_data/nq_open_validation_raw.json \\\n",
    "    --output_file /workspace/ragu/colab_data/nq_open_validation.jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_input_path = DATA_DIR / 'nq_open_validation_raw.json'\n",
    "processed_input_path = DATA_DIR / 'nq_open_validation.jsonl'\n",
    "print('Raw DPR JSON:', raw_input_path)\n",
    "print('Processed retrieval input stored at', processed_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Download Contriever passages and embeddings\n",
    "Download the Wikipedia passages plus the pre-built Contriever embeddings into the shared cache directory using the Docker image."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "CACHE_ROOT=/content/ragu_cache/contriever\n",
    "mkdir -p ${CACHE_ROOT}\n",
    "\n",
    "docker run --gpus all --rm \\\n",
    "  -v /content/ragu_cache:/workspace/cache \\\n",
    "  -w /workspace/cache/contriever \\\n",
    "  lauhaide/algomo:v1 \\\n",
    "  /bin/bash -lc \"set -e\n",
    "mkdir -p /workspace/cache/contriever\n",
    "cd /workspace/cache/contriever\n",
    "if [ ! -f psgs_w100.tsv.gz ]; then\n",
    "  wget -q https://dl.fbaipublicfiles.com/dpr/wikipedia_split/psgs_w100.tsv.gz\n",
    "fi\n",
    "if [ ! -f wikipedia_embeddings.tar ]; then\n",
    "  wget -q https://dl.fbaipublicfiles.com/contriever/embeddings/contriever-msmarco/wikipedia_embeddings.tar\n",
    "fi\n",
    "if [ ! -d wikipedia_embeddings ]; then\n",
    "  mkdir -p wikipedia_embeddings\n",
    "  tar -xf wikipedia_embeddings.tar -C wikipedia_embeddings --strip-components=1\n",
    "fi\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run dense retrieval (Contriever)\n",
    "Execute the Contriever retrieval script from inside Docker so that dependencies such as FAISS are satisfied by the image."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "DATA_PATH=/content/ragu/colab_data/nq_open_validation.jsonl\n",
    "OUTPUT_PATH=/content/ragu/colab_data/nq_open_validation_retrieved.jsonl\n",
    "\n",
    "docker run --gpus all --rm \\\n",
    "  -v /content/ragu:/workspace/ragu \\\n",
    "  -v /content/contriever:/workspace/contriever \\\n",
    "  -v /content/ragu_cache:/workspace/cache \\\n",
    "  -w /workspace/contriever/scripts \\\n",
    "  lauhaide/algomo:v1 \\\n",
    "  python passage_retrieval.py \\\n",
    "    --model_name_or_path facebook/contriever-msmarco \\\n",
    "    --passages /workspace/cache/contriever/psgs_w100.tsv.gz \\\n",
    "    --passages_embeddings \"/workspace/cache/contriever/wikipedia_embeddings/*\" \\\n",
    "    --data /workspace/ragu/colab_data/nq_open_validation.jsonl \\\n",
    "    --output_dir /workspace/ragu/colab_data/nq_open_validation_retrieved.jsonl \\\n",
    "    --n_docs 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run RAG answer generation with `run_baseline_lm.py`\n",
    "Launch the generator inside Docker (vLLM is already included in the image). You can change `GENERATOR_MODEL` to use a smaller model if VRAM is limited."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "cd /content/ragu\n",
    "GENERATOR_MODEL=google/gemma-2-9b-it\n",
    "INPUT_FILE=colab_data/nq_open_validation_retrieved.jsonl\n",
    "OUTPUT_FILE=colab_outputs/gemma2_nq_validation.jsonl\n",
    "\n",
    "docker run --gpus all --rm \\\n",
    "  -e HF_TOKEN=\"${HF_TOKEN}\" \\\n",
    "  -e HF_HOME=/workspace/cache/huggingface \\\n",
    "  -v /content/ragu:/workspace/ragu \\\n",
    "  -v /content/ragu_cache:/workspace/cache \\\n",
    "  -w /workspace/ragu \\\n",
    "  lauhaide/algomo:v1 \\\n",
    "  python retrieval_qa/run_baseline_lm.py \\\n",
    "    --model_name ${GENERATOR_MODEL} \\\n",
    "    --split validation \\\n",
    "    --input_file ${INPUT_FILE} \\\n",
    "    --result_fp ${OUTPUT_FILE} \\\n",
    "    --prompt_name chat_directRagQA_REAR3 \\\n",
    "    --chat_template \\\n",
    "    --top_n 5 \\\n",
    "    --temperature 0.0 \\\n",
    "    --top_p 1.0 \\\n",
    "    --max_new_tokens 50 \\\n",
    "    --logprobs 1 \\\n",
    "    --compute_pmi \\\n",
    "    --batch_size 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Score answers with `run_compute_accLM.py`\n",
    "The judge model also runs from inside the Docker image. Adjust `EVAL_MODEL` or `--batch_size` if you hit memory limits."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "cd /content/ragu\n",
    "EVAL_MODEL=Qwen/Qwen2-1.5B-Instruct\n",
    "PRED_FILE=colab_outputs/gemma2_nq_validation.jsonl\n",
    "SCORED_FILE=colab_outputs/gemma2_nq_validation_acclm.jsonl\n",
    "\n",
    "docker run --gpus all --rm \\\n",
    "  -e HF_TOKEN=\"${HF_TOKEN}\" \\\n",
    "  -e HF_HOME=/workspace/cache/huggingface \\\n",
    "  -v /content/ragu:/workspace/ragu \\\n",
    "  -v /content/ragu_cache:/workspace/cache \\\n",
    "  -w /workspace/ragu \\\n",
    "  lauhaide/algomo:v1 \\\n",
    "  python retrieval_qa/run_compute_accLM.py \\\n",
    "    --model_name ${EVAL_MODEL} \\\n",
    "    --input_file ${PRED_FILE} \\\n",
    "    --result_fp ${SCORED_FILE} \\\n",
    "    --acc \\\n",
    "    --top_n 5 \\\n",
    "    --batch_size 4 \\\n",
    "    --prompt_name chat_accuracy_eval-rlhf-calib \\\n",
    "    --chat_template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inspect aggregate metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "scored_path = OUTPUT_DIR / 'gemma2_nq_validation_acclm.jsonl'\n",
    "entries = list(jsonlines.open(scored_path))\n",
    "acc_values = [entry.get('acc_LM', 0) for entry in entries]\n",
    "print('Total examples:', len(entries))\n",
    "print('LM-judged accuracy:', np.mean(acc_values))\n",
    "print('Sample prediction:', entries[0]['output'])\n",
    "print('Sample gold answers:', entries[0]['golds'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next steps\n",
    "* Swap in alternative generator or judge models by editing the variables in Sections 7\u20138.\n",
    "* Increase `--n_docs` or `--top_n` to match the paper's retrieval depth.\n",
    "* Persist `colab_data` and `colab_outputs` folders to Google Drive if you need to reuse them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}